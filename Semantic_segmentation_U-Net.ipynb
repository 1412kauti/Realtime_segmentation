{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cc31d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "def multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1):\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    \n",
    "    # Define encoder layers\n",
    "    c1 = conv_block(inputs, 16)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = conv_block(p1, 32)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = conv_block(p2, 64)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    \n",
    "    c4 = conv_block(p3, 128)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    \n",
    "    c5 = conv_block(p4, 256)\n",
    "    \n",
    "    # Define decoder layers\n",
    "    u6 = upconv_block(c5, c4, 128)\n",
    "    u7 = upconv_block(u6, c3, 64)\n",
    "    u8 = upconv_block(u7, c2, 32)\n",
    "    u9 = upconv_block(u8, c1, 16)\n",
    "    \n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(u9)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "def conv_block(inputs, filters):\n",
    "    conv1 = Conv2D(filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    conv1 = Dropout(0.1)(conv1)\n",
    "    conv2 = Conv2D(filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)\n",
    "    return conv2\n",
    "\n",
    "def upconv_block(inputs, concat_layer, filters):\n",
    "    upconv = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(inputs)\n",
    "    upconv = concatenate([upconv, concat_layer])\n",
    "    conv1 = Conv2D(filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(upconv)\n",
    "    conv1 = Dropout(0.1)(conv1)\n",
    "    conv2 = Conv2D(filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)\n",
    "    return conv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90eda441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Navdeep\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Class values in the dataset are ...  [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "class weights dict\n",
      "{0: 0.49207878217925755, 1: 0.3577460691552256, 2: 8.480282631528693, 3: 0.2643396243613218, 4: 1.853555299679677, 5: 0.8547461368653422, 6: 7.099427146856614, 7: 7.388813494106208, 8: 1.422141625549979, 9: 13.126954489451155, 10: 28.599441002070456, 11: 2.1202343557957253}\n",
      "Class weights are...: [ 0.49207878  0.35774607  8.48028263  0.26433962  1.8535553   0.85474614\n",
      "  7.09942715  7.38881349  1.42214163 13.12695449 28.599441    2.12023436]\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 352, 352, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)          (None, 352, 352, 16)         160       ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 352, 352, 16)         0         ['conv2d_76[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)          (None, 352, 352, 16)         2320      ['dropout_36[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooli  (None, 176, 176, 16)         0         ['conv2d_77[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)          (None, 176, 176, 32)         4640      ['max_pooling2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 176, 176, 32)         0         ['conv2d_78[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)          (None, 176, 176, 32)         9248      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooli  (None, 88, 88, 32)           0         ['conv2d_79[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)          (None, 88, 88, 64)           18496     ['max_pooling2d_17[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)        (None, 88, 88, 64)           0         ['conv2d_80[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)          (None, 88, 88, 64)           36928     ['dropout_38[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooli  (None, 44, 44, 64)           0         ['conv2d_81[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)          (None, 44, 44, 128)          73856     ['max_pooling2d_18[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)        (None, 44, 44, 128)          0         ['conv2d_82[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)          (None, 44, 44, 128)          147584    ['dropout_39[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooli  (None, 22, 22, 128)          0         ['conv2d_83[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)          (None, 22, 22, 256)          295168    ['max_pooling2d_19[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)        (None, 22, 22, 256)          0         ['conv2d_84[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)          (None, 22, 22, 256)          590080    ['dropout_40[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_16 (Conv2  (None, 44, 44, 128)          131200    ['conv2d_85[0][0]']           \n",
      " DTranspose)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenat  (None, 44, 44, 256)          0         ['conv2d_transpose_16[0][0]', \n",
      " e)                                                                  'conv2d_83[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)          (None, 44, 44, 128)          295040    ['concatenate_16[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)        (None, 44, 44, 128)          0         ['conv2d_86[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)          (None, 44, 44, 128)          147584    ['dropout_41[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_17 (Conv2  (None, 88, 88, 64)           32832     ['conv2d_87[0][0]']           \n",
      " DTranspose)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenat  (None, 88, 88, 128)          0         ['conv2d_transpose_17[0][0]', \n",
      " e)                                                                  'conv2d_81[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)          (None, 88, 88, 64)           73792     ['concatenate_17[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)        (None, 88, 88, 64)           0         ['conv2d_88[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)          (None, 88, 88, 64)           36928     ['dropout_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_18 (Conv2  (None, 176, 176, 32)         8224      ['conv2d_89[0][0]']           \n",
      " DTranspose)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenat  (None, 176, 176, 64)         0         ['conv2d_transpose_18[0][0]', \n",
      " e)                                                                  'conv2d_79[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)          (None, 176, 176, 32)         18464     ['concatenate_18[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)        (None, 176, 176, 32)         0         ['conv2d_90[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)          (None, 176, 176, 32)         9248      ['dropout_43[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_19 (Conv2  (None, 352, 352, 16)         2064      ['conv2d_91[0][0]']           \n",
      " DTranspose)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenat  (None, 352, 352, 32)         0         ['conv2d_transpose_19[0][0]', \n",
      " e)                                                                  'conv2d_77[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)          (None, 352, 352, 16)         4624      ['concatenate_19[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)        (None, 352, 352, 16)         0         ['conv2d_92[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)          (None, 352, 352, 16)         2320      ['dropout_44[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 352, 352, 12)         204       ['conv2d_93[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1941004 (7.40 MB)\n",
      "Trainable params: 1941004 (7.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\Navdeep\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Navdeep\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 5/21 [======>.......................] - ETA: 38s - loss: 2.5225 - accuracy: 0.0840"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 131\u001b[0m\n\u001b[0;32m    123\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# # If starting with pre-trained weights\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# # model.load_weights('???.hdf5')\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# # Train the model with callbacks\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# print(X_train.shape)\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# print(y_train_cat.shape)\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_cat, \n\u001b[0;32m    132\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \n\u001b[0;32m    133\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m    134\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[0;32m    135\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test_cat), \n\u001b[0;32m    136\u001b[0m                     class_weight\u001b[38;5;241m=\u001b[39mclass_weights_dict,  \u001b[38;5;66;03m# Uncomment if you want to use class weights\u001b[39;00m\n\u001b[0;32m    137\u001b[0m                     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    138\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[model_checkpoint , early_stopping])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import normalize\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "SIZE_X = 352 \n",
    "SIZE_Y = 352\n",
    "n_classes = 12  # Number of classes for segmentation\n",
    "train_images = []\n",
    "\n",
    "directory_path = r\"C:\\UNET_project\\camvid\\train\"\n",
    "\n",
    "# Using list comprehension for a more compact code\n",
    "train_images = [cv2.resize(cv2.imread(os.path.join(directory_path, filename), 0), (SIZE_Y, SIZE_X))\n",
    "                for filename in os.listdir(directory_path)\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png')]\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "\n",
    "directory_path_masks = r\"C:\\UNET_project\\camvid\\trainannot\"\n",
    "\n",
    "# Using list comprehension for a more compact code\n",
    "train_masks = [cv2.resize(cv2.imread(os.path.join(directory_path_masks, filename), 0), (SIZE_Y, SIZE_X), interpolation=cv2.INTER_NEAREST)\n",
    "               for filename in os.listdir(directory_path_masks)\n",
    "               if filename.endswith('.jpg') or filename.endswith('.png')]\n",
    "\n",
    "# Convert list to array for machine learning processing\n",
    "train_masks = np.array(train_masks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for label encoding\n",
    "def label_encode_masks(train_masks):\n",
    "    labelencoder = LabelEncoder()\n",
    "    n, h, w = train_masks.shape\n",
    "    train_masks_reshaped = train_masks.reshape(-1, 1)\n",
    "    train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "    train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "    return train_masks_encoded_original_shape , train_masks_reshaped_encoded\n",
    "\n",
    "# Encode the masks\n",
    "train_masks_encoded_original_shape , train_masks_reshaped_encoded = label_encode_masks(train_masks)\n",
    "\n",
    "# Print unique values and the array\n",
    "print(\"Unique values:\", np.unique(train_masks_encoded_original_shape))\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(train_images, train_masks):\n",
    "    # Preprocess images\n",
    "    train_images_processed = np.expand_dims(train_images, axis=3)\n",
    "    train_images_processed = normalize(train_images_processed, axis=1)\n",
    "    \n",
    "    # Preprocess masks\n",
    "    train_masks_processed = np.expand_dims(train_masks, axis=3)\n",
    "    \n",
    "    return train_images_processed, train_masks_processed\n",
    "\n",
    "# Preprocess data\n",
    "train_images, train_masks_input = preprocess_data(train_images, train_masks_encoded_original_shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_images, train_masks, test_size=0.10, random_state=0)\n",
    "\n",
    "# Print unique class values in the training dataset\n",
    "print(\"Class values in the dataset are ... \", np.unique(y_train))\n",
    "\n",
    "# Function to convert masks to categorical format\n",
    "def convert_to_categorical(masks, num_classes):\n",
    "    masks_cat = to_categorical(masks, num_classes=num_classes)\n",
    "    return masks_cat.reshape((*masks.shape, num_classes)) , masks_cat\n",
    "\n",
    "# Convert training masks to categorical format\n",
    "y_train_cat , train_masks_cat = convert_to_categorical(y_train, num_classes=n_classes)\n",
    "\n",
    "# Convert testing masks to categorical format\n",
    "y_test_cat , test_masks_cat = convert_to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "classes = np.unique(train_masks_reshaped_encoded)\n",
    "class_weights = compute_class_weight( class_weight= \"balanced\" , classes =  np.unique(train_masks_reshaped_encoded), y=train_masks_reshaped_encoded)\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "print('class weights dict')\n",
    "print(class_weights_dict)\n",
    "print(\"Class weights are...:\", class_weights)\n",
    "\n",
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH  = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]\n",
    "\n",
    "def get_model():\n",
    "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
    "\n",
    "model = get_model()\n",
    "model_checkpoint = ModelCheckpoint(filepath=r'C:\\UNET_project\\weights_2\\model_epoch_{epoch:02d}.h5',  \n",
    "                                   monitor='val_loss',  \n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False,  \n",
    "                                   save_weights_only=False,  \n",
    "                                   mode='auto',  \n",
    "                                   period=1)\n",
    "\n",
    "# # Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',  # Monitor validation accuracy\n",
    "                               min_delta=0.001,  # Minimum change to qualify as an improvement\n",
    "                               patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "                               verbose=1,  \n",
    "                               mode='auto')  \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "model.summary()\n",
    "\n",
    "# # If starting with pre-trained weights\n",
    "# # model.load_weights('???.hdf5')\n",
    "\n",
    "# # Train the model with callbacks\n",
    "# print(X_train.shape)\n",
    "# print(y_train_cat.shape)\n",
    "history = model.fit(X_train, y_train_cat, \n",
    "                    batch_size=16, \n",
    "                    verbose=1, \n",
    "                    epochs=50, \n",
    "                    validation_data=(X_test, y_test_cat), \n",
    "                    class_weight=class_weights_dict,  # Uncomment if you want to use class weights\n",
    "                    shuffle=False,\n",
    "                    callbacks=[model_checkpoint , early_stopping])\n",
    "                    \n",
    "\n",
    "model.save(r'C:\\UNET_project\\weights_2\\test.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "195b4520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 172ms/step\n",
      "(37, 352, 352, 12)\n",
      "<class 'numpy.ndarray'>\n",
      "Mean IoU = 0.4041226\n",
      "[0.85698843, 0.47007307, 0.0725492, 0.8856767, 0.43509936, 0.52067554, 0.10914341, 0.12037788, 0.67477536, 0.14755052, 0.34439555, 0.21216625]\n"
     ]
    }
   ],
   "source": [
    "# _, acc = model.evaluate(X_test, y_test_cat)\n",
    "# print(\"Accuracy is = \", (acc * 100.0), \"%\")\n",
    "\n",
    "\n",
    "# # ###\n",
    "# # #plot the training and validation accuracy and loss at each epoch\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "# plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "\n",
    "# plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "# plt.title('Training and validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ##################################\n",
    "model = get_model()\n",
    "model.load_weights(r\"C:\\UNET_project\\weights_camvid\\test.hdf5\")  \n",
    "# #model.load_weights('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')  \n",
    "\n",
    "#IOU\n",
    "y_pred=model.predict(X_test)\n",
    "print(y_pred.shape)\n",
    "print(type(y_pred))\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
    "# print(y_pred_argmax)\n",
    "\n",
    "# # ##################################################\n",
    "\n",
    "# # #Using built in keras function\n",
    "from keras.metrics import MeanIoU\n",
    "n_classes = 12\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "# #To calculate I0U for each class...\n",
    "# values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "# print(values)\n",
    "# class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "# class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "# class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "# class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "# values = np.array(IOU_keras.get_weights()).reshape(12, 12)\n",
    "\n",
    "class_IoU = []\n",
    "\n",
    "# for i in range(12):\n",
    "#     # Calculate IoU for each class\n",
    "#     numerator = values[i, i]\n",
    "#     denominator = np.sum(values[i, :]) + np.sum(values[:, i]) - values[i, i]\n",
    "#     class_i_IoU = numerator / denominator\n",
    "#     class_IoU.append(class_i_IoU)\n",
    "\n",
    "# print(class_IoU)    \n",
    "# print(\"Class IoU:\", class_IoU)\n",
    "# print(\"IoU for class1 is: \", class1_IoU)\n",
    "# print(\"IoU for class2 is: \", class2_IoU)\n",
    "# print(\"IoU for class3 is: \", class3_IoU)\n",
    "# print(\"IoU for class4 is: \", class4_IoU)\n",
    "\n",
    "plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
    "plt.imshow(train_masks[0], cmap='gray')\n",
    "#######################################################################\n",
    "#Predict on a few images\n",
    "#model = get_model()\n",
    "#model.load_weights('???.hdf5')  \n",
    "import random\n",
    "test_img_number = random.randint(0, len(X_test))\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "test_img_norm=test_img[:,:,0][:,:,None]\n",
    "test_img_input=np.expand_dims(test_img_norm, 0)\n",
    "prediction = (model.predict(test_img_input))\n",
    "predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(predicted_img, cmap='jet')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #Predict on large image\n",
    "\n",
    "# #Apply a trained model on large image\n",
    "\n",
    "# from patchify import patchify, unpatchify\n",
    "\n",
    "# large_image = cv2.imread('large_images/large_image.tif', 0)\n",
    "# #This will split the image into small images of shape [3,3]\n",
    "# patches = patchify(large_image, (128, 128), step=128)  #Step=256 for 256 patches means no overlap\n",
    "\n",
    "# predicted_patches = []\n",
    "# for i in range(patches.shape[0]):\n",
    "#     for j in range(patches.shape[1]):\n",
    "#         print(i,j)\n",
    "        \n",
    "#         single_patch = patches[i,j,:,:]       \n",
    "#         single_patch_norm = np.expand_dims(normalize(np.array(single_patch), axis=1),2)\n",
    "#         single_patch_input=np.expand_dims(single_patch_norm, 0)\n",
    "#         single_patch_prediction = (model.predict(single_patch_input))\n",
    "#         single_patch_predicted_img=np.argmax(single_patch_prediction, axis=3)[0,:,:]\n",
    "\n",
    "#         predicted_patches.append(single_patch_predicted_img)\n",
    "\n",
    "# predicted_patches = np.array(predicted_patches)\n",
    "\n",
    "# predicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], 128,128) )\n",
    "\n",
    "# reconstructed_image = unpatchify(predicted_patches_reshaped, large_image.shape)\n",
    "# plt.imshow(reconstructed_image, cmap='gray')\n",
    "# plt.imsave('data/results/segm.jpg', reconstructed_image, cmap='gray')\n",
    "\n",
    "# plt.hist(reconstructed_image.flatten())  #Threshold everything above 0\n",
    "\n",
    "# final_prediction = (reconstructed_image > 0.01).astype(np.uint8)\n",
    "# plt.imshow(final_prediction)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(221)\n",
    "# plt.title('Large Image')\n",
    "# plt.imshow(large_image, cmap='gray')\n",
    "# plt.subplot(222)\n",
    "# plt.title('Prediction of large Image')\n",
    "# plt.imshow(reconstructed_image, cmap='jet')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5cf19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6191f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
